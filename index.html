<!DOCTYPE html>
<html>
  <head>
    <title>SPEAR-TTS</title>
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css"
      rel="stylesheet"
    />
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
    <script src="helper.js" defer></script>
    <style>
      td {
        vertical-align: middle;
      }
      audio {
        width: 20vw;
        min-width: 100px;
        max-width: 250px;
      }
    </style>
  </head>
  <body>
    <div class="container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded">
      <div class="text-center">
        <h2>TODO TITLE</h2>
        <p class="lead fw-bold">
          |<a
            href="TODO PAPER LINK"
            class="btn border-white bg-white fw-bold"
            >paper</a
          >|
        </p>
        <p class="fst-italic mb-0">
          TODO AUTHOR NAMES 
        </p>
        <p><b>Google Research</b></p>
      </div>
      <p>
        <b>Abstract.</b>  
        We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to "reading") and from semantic tokens to low-level acoustic tokens ("speaking"). Decoupling these two tasks enables training of the "speaking" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the "reading" component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests.


      </p>
    </div>

<div class="container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded">
    <h2 id="model-overview" style="text-align: center;">Model Overview</h2>
    <body>
    <p style="text-align: center;">
        <img src="arch.png" height="200" width="800" class="img-fluid">
    </p>
    </body>
        <p>
TODO ARCHITECTURE DESCRIPTION
        </p>
</div>

<!-- <div class="container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded">
    <h2 id="model-overview" style="text-align: left;">Table of Contents</h2>
    <body>
    <p style="text-align: left;">
    <ul style="list-style: outside none none !important;">
       <li><a href="#efficiency" class="btn border-white bg-white fw-bold">Supervision Efficiency</a></li>
       <li><a href="#diversity" class="btn border-white bg-white fw-bold">Speech Diversity</a></li>
       <li><a href="#prompting" class="btn border-white bg-white fw-bold">Voice Prompting</a></li>
       <li><a href="#impact" class="btn border-white bg-white fw-bold">Broader Impact</a></li>
    </ul>
    </p>
    </body>
</div> -->



    <div class="container shadow p-5 mb-5 bg-white rounded">
      <h3>Voice Conversion<a id="prompting"/></h3>
      <p class="mb-0">
      In the following set of samples we show that, when prompted with a 3-second sample, SPEAR-TTS generates an utterance that is faithful to the prompted voice. Here, voices are selected from LibriSpeech test-clean (Panayotov et al., 2015), so the model has never seen them during training, thus demonstrating zero-shot generalization. We use the SPEAR-TTS model trained on a 15 minute subset of LJSpeech (single-speaker) as parallel data.
      </p>
      <div class="container pt-3 table-responsive">
        <table
          class="table table-hover"
          id="prompting-table"
        >
          <thead>
            <tr>
              <th width="20%">Prompt</th>
              <th width="20%">Sample #1</th>
              <th width="20%">Sample #2</th>
              <th width="20%">Sample #3</th>
              <th width="20%">Sample #4</th>
            </tr>
          </thead>
          <tbody>
             <tr>
               <td colspan="5" style="text-align: center">Text: <font size="-1">
                   "And yesterday things went on just as usual"
               </font></td>
             </tr>
             <tr> <td>Speaker #1 Prompt #1</td> <td></td> <td></td> <td></td> <td></td> </tr>
             <tr> <td>Speaker #1 Prompt #2</td> <td></td> <td></td> <td></td> <td></td> </tr>
             <tr> <td>Speaker #2 Prompt #1</td> <td></td> <td></td> <td></td> <td></td> </tr>
             <tr> <td>Speaker #2 Prompt #2</td> <td></td> <td></td> <td></td> <td></td> </tr>
             <tr>
               <td colspan="5" style="text-align: center">Text: <font size="-1">
                   "I see a crowd in one corner of the garden everybody standing still and looking up"
               </font></td>
             </tr>
             <tr> <td>Speaker #1 Prompt #1</td> <td></td> <td></td> <td></td> <td></td> </tr>
             <tr> <td>Speaker #1 Prompt #2</td> <td></td> <td></td> <td></td> <td></td> </tr>
             <tr> <td>Speaker #2 Prompt #1</td> <td></td> <td></td> <td></td> <td></td> </tr>
             <tr> <td>Speaker #2 Prompt #2</td> <td></td> <td></td> <td></td> <td></td> </tr>
          </tbody>
        </table>
      </div>
    </div>


    <div class="container shadow p-5 mb-5 bg-white rounded">
      <h3>Broader impact<a id="impact"/></h3>
      <p class="mb-0">
      We believe our work on high-quality TTS that requires very limited supervision (quantity- and quality-wise) can be an important stepping stone for enabling this core speech technology for communities that are currently underserved by TTS solutions due to speaking "low-resource" languages, i.e., languages do not have vast parallel corpora required for training deep learning models. Even for high-resource languages, such as English, the ability to harness unpaired data for speech generation can enable producing speech in accents and dialects that are currently uncovered by the existing TTS systems.
      </p>
      <br />
      <p>
      At the same time, we acknowledge that the ability to mimic a voice can have numerous malicious applications, including bypassing biometric identification for the purpose of impersonation. Thus it is crucial to put safeguards against the misuse of SPEAR-TTS and, as an initial step, we verify that speech produced by SPEAR-TTS can be reliably detected by a classifier (see Appendix E). In the future, one can explore other approaches for detecting synthesized speech, for example by audio watermarking.
     </p>
    </div>



  </body>
</html>
