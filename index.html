<!DOCTYPE html>
<html>
  <head>
    <title>Textless Voice Conversion with Normalized Discrete Units</title>
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css"
      rel="stylesheet"
    />
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
    <script src="helper.js" defer></script>
    <style>
      td {
        vertical-align: middle;
      }
      audio {
        width: 20vw;
        min-width: 100px;
        max-width: 250px;
      }
    </style>
  </head>
  <body>
    <div class="container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded">
      <div class="text-center">
        <h2>Textless Voice Conversion with Normalized Discrete Units</h2>
        <p class="lead fw-bold">
<!--           |<a
            href="TODO PAPER LINK"
            class="btn border-white bg-white fw-bold"
            >paper</a
          >| -->
        </p>
        <p class="fst-italic mb-0">
          Alan Baade, Puyuan Peng, David Harwath 
        </p>
        <p><b>The University of Texas at Austin</b></p>
        <p><b>Paper Available by Request</b></p>
      </div>
      <p>
        <b>Abstract.</b>  
We introduce a method for textless any-to-any voice conversion based on the recent progress in speech synthesis driven by neural codec language models. To disentangle the speaker and linguistic information, we adapt a speaker normalizing procedure for discrete semantic units, and then generate with an autoregressive language model for greatly improved diversity. We further improve the similarity of the output audio to the target speaker's voice by leveraging classifier free guidance. We evaluate our techniques against current text to speech synthesis and voice conversion systems and compare the effectiveness of different neural codec language model pipelines. We demonstrate state-of-the-art results in speaker similarity with significantly less compute than existing codec language models such as VALL-E.

      </p>
    </div>

<!-- <div class="container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded">
    <h2 id="model-overview" style="text-align: center;">Model Overview</h2>
    <body>
    <p style="text-align: center;">
        <img src="arch.png" height="200" width="800" class="img-fluid">
    </p>
    </body>
        <p>
TODO ARCHITECTURE DESCRIPTION
        </p>
</div> -->

<!-- <div class="container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded">
    <h2 id="model-overview" style="text-align: left;">Table of Contents</h2>
    <body>
    <p style="text-align: left;">
    <ul style="list-style: outside none none !important;">
       <li><a href="#efficiency" class="btn border-white bg-white fw-bold">Supervision Efficiency</a></li>
       <li><a href="#diversity" class="btn border-white bg-white fw-bold">Speech Diversity</a></li>
       <li><a href="#prompting" class="btn border-white bg-white fw-bold">Voice Prompting</a></li>
       <li><a href="#impact" class="btn border-white bg-white fw-bold">Broader Impact</a></li>
    </ul>
    </p>
    </body>
</div> -->



  <div class="container shadow p-5 mb-5 bg-white rounded">
    <h3>Voice Conversion on Unseen Speakers from LibriSpeech Test-Clean<a id="prompting"/></h3>
    <p class="mb-0">
    Cherrypicked examples for Voice Conversion. All speakers are unseen to all models during training.
    </p>
    <div class="container pt-3 table-responsive">
      <table
        class="table table-hover"
        id="librispeech-test-table"
      >
        <thead>
          <tr>
            <th width="12.5%">Source</th>
            <th width="12.5%">Target</th>
            <th width="12.5%">Ours-Norm</th>
            <th width="12.5%">Ours-Aligned</th>
            <th width="12.5%">Ours-Phone</th>
            <th width="12.5%">TriAAN-VC</th>
            <th width="12.5%">FreeVC</th>
            <th width="12.5%">DiffVC</th>
          </tr>
        </thead>
        <tbody>
           <tr> <td>SID 4446 to 2300</td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td>  </tr>
           <tr> <td>SID 61 to 2830</td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td>  </tr>
           <tr> <td>SID 121 to 4992</td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td>  </tr>
           <tr> <td>SID 1320 to 61</td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td>  </tr>
           <tr> <td>SID 1995 to 5639</td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td>  </tr>
           <tr> <td>SID 4507 to 2300</td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td>  </tr>
           <tr> <td>SID 6930 to 2961</td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td>  </tr>
           <tr> <td>SID 7176 to 8555</td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td>  </tr>
        </tbody>
      </table>
    </div>
  </div>

  <div class="container shadow p-5 mb-5 bg-white rounded">
    <h3>Voice Conversion on Speakers from VCTK. <a id="prompting"/></h3>
    <p class="mb-0">
    Cherrypicked examples for Voice Conversion on VCTK. Speakers are unseen during training for our models and TriAAN-VC. Speakers are possibly seen by FreeVC and DiffVC during training.
    <br>
    To demonstrate the effect of accent, these examples largely convert between speakers labeled as American or Canadian and speakers who aren't. 
    <br>
    Notice that Ours-Norm converts between accents, while the source speaker's accent is almost entirely preserved for Ours-Aligned, TriAAN-VC, FreeVC, and DiffVC.
    </p>
    <div class="container pt-3 table-responsive">
      <table
        class="table table-hover"
        id="vctk-test-table"
      >
        <thead>
          <tr>
            <th width="12.5%">Source</th>
            <th width="12.5%">Target</th>
            <th width="12.5%">Ours-Norm</th>
            <th width="12.5%">Ours-Aligned</th>
            <th width="12.5%">Ours-Phone</th>
            <th width="12.5%">TriAAN-VC</th>
            <th width="12.5%">FreeVC*</th>
            <th width="12.5%">DiffVC*</th>
          </tr>
        </thead>
        <tbody>
           <tr> <td>SID p277 to p299</td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td>  </tr>
           <tr> <td>SID p294 to p231</td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td>  </tr>
           <tr> <td>SID p227 to p308</td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td>  </tr>
           <tr> <td>SID p245 to p299</td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td>  </tr>
           <tr> <td>SID p316 to p249</td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td>  </tr>
           <tr> <td>SID p343 to p285</td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td>  </tr>
           <tr> <td>SID p330 to p336</td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td>  </tr>
           <tr> <td>SID p271 to p285</td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td> <td></td>  </tr>
        </tbody>
      </table>
    </div>
      
  </div>

    Website layout taken from the SPEAR-TTS Demo Page

  </body>
</html>
